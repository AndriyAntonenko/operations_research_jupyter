{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from sympy import Float, init_printing, Array, relational, symbols, Matrix\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def target_function(x1,x2):\n",
    "    return 2*(x1 - 1)**2 + x1*x2 + 6*x2**2\n",
    "\n",
    "\n",
    "def df_dx1(x1, x2):\n",
    "    return 4*x1 + x2 - 4\n",
    "\n",
    "\n",
    "def df_dx2(x1, x2):\n",
    "    return x1 + 12*x2\n",
    "\n",
    "\n",
    "ANALYTICAL_GRADIENT = [df_dx1, df_dx2]\n",
    "START_POINT = [5.2, 5.2]\n",
    "ANALYTICAL_HESSE = [[lambda x, y: 4, lambda x, y: 1], [lambda x, y: 1, lambda x, y: 12]]\n",
    "\n",
    "x_min, f_min = symbols('x_min f_min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(analytical_gradient, point):\n",
    "    \"\"\"\n",
    "    :param analytical_gradient: list of lambdas (analytical gradient)\n",
    "    :param point:               list of numbers (current point)\n",
    "    :return:                    list of numbers (calculated gradient)\n",
    "    \"\"\"\n",
    "    values_gradient = []\n",
    "    for i in range(len(analytical_gradient)):\n",
    "        values_gradient.append(analytical_gradient[i](*point))\n",
    "    return values_gradient\n",
    "\n",
    "\n",
    "def calculate_anti_gradient(gradient):\n",
    "    \"\"\"\n",
    "    :param gradient: list of numbers (gradient values)\n",
    "    :return:         list of numbers (anti-gradient values)\n",
    "    \"\"\"\n",
    "    anti_gradient = []\n",
    "    for i in range(len(gradient)):\n",
    "        anti_gradient.append(-1 * gradient[i])\n",
    "    return anti_gradient\n",
    "\n",
    "\n",
    "def calculate_gradient_norm(gradient):\n",
    "    res = 0\n",
    "    for i in range(len(gradient)):\n",
    "        res += gradient[i]**2\n",
    "    return sqrt(res)\n",
    "\n",
    "\n",
    "def calculate_optimal_step(s_vector, gradient_values, hesse):\n",
    "    \"\"\"\n",
    "    :param s_vector:        list of numbers       (the direction of the greatest decline of the function)\n",
    "    :param gradient_values: list of numbers       (gradient in point)\n",
    "    :param hesse:           multidimensional list (hesse matrix)\n",
    "    :return:                number                (optimal step)\n",
    "    \"\"\"\n",
    "    s = np.array(s_vector)\n",
    "    g = np.array(gradient_values)\n",
    "    h = np.array(hesse)\n",
    "\n",
    "    a = g.transpose().dot(s)\n",
    "    b = s.transpose().dot(h).dot(s)\n",
    "\n",
    "    return -1 * a / b\n",
    "\n",
    "\n",
    "def calculate_hesse(analytical_hesse, point):\n",
    "    result = []\n",
    "    for i in range(len(analytical_hesse)):\n",
    "        result.append([])\n",
    "        for j in range(len(analytical_hesse[i])):\n",
    "            result[i].append(analytical_hesse[i][j](*point))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Fletcher-Rives's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_new_direction_fr(anti_gradient, prev_gradient, current_gradient, prev_direction):\n",
    "    \"\"\"\n",
    "    Calculating of new direction by Fletcher-Rives' method\n",
    "    :param anti_gradient:       list of numbers (anti-gradient values)\n",
    "    :param prev_gradient:       list of numbers (gradient values in prev point)\n",
    "    :param current_gradient:    list of numbers (gradient values in current point)\n",
    "    :param prev_direction:      list of numbers (direction vector in prev point)\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ag = np.array(anti_gradient, dtype=float)\n",
    "    pg = np.array(prev_gradient, dtype=float)\n",
    "    cg = np.array(current_gradient, dtype=float)\n",
    "    pd = np.array(prev_direction, dtype=float)\n",
    "\n",
    "    a = cg.transpose().dot(cg)\n",
    "    b = pg.transpose().dot(pg)\n",
    "    if b == 0:\n",
    "        return []\n",
    "    return np.add(ag, (a / b) * pd)\n",
    "\n",
    "\n",
    "def fletcher_rives_method(analytical_gradient, start_point, hesse, epsilon):\n",
    "    \"\"\"\n",
    "    Minimization of function by Fletcher-Rives' method\n",
    "    :param analytical_gradient: list of lambdas     (analytical gradient)\n",
    "    :param start_point:         list of numbers     (start point)\n",
    "    :param hesse:               2d list of lambdas  (analytical hesse' matrix)\n",
    "    :param epsilon:             float               (accuracy)\n",
    "    :return:                    list of points and list of gradients norms for each iteration\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    gradient_norms = []\n",
    "    point = start_point.copy()\n",
    "    gradient = calculate_gradient(analytical_gradient, point)\n",
    "    direction = calculate_anti_gradient(gradient)\n",
    "    step = calculate_optimal_step(direction, gradient, calculate_hesse(hesse, point))\n",
    "    gradient_norm = np.linalg.norm(np.array(gradient))\n",
    "\n",
    "    points.append(point)\n",
    "    gradient_norms.append(gradient_norm)\n",
    "\n",
    "    while gradient_norm > epsilon:\n",
    "        point =  np.add(np.array(point), (step * np.array(direction)))\n",
    "        new_gradient = calculate_gradient(analytical_gradient, point)\n",
    "        new_anti_gradient = calculate_anti_gradient(new_gradient)\n",
    "\n",
    "        direction = calculate_new_direction_fr(new_anti_gradient, gradient, new_gradient, direction)\n",
    "        if len(direction) == 0:\n",
    "            print(\"To small value of gradient norm\")\n",
    "            break\n",
    "        gradient = new_gradient\n",
    "        gradient_norm = np.linalg.norm(np.array(new_gradient))\n",
    "        step = calculate_optimal_step(direction, gradient, calculate_hesse(hesse, point))\n",
    "        gradient_norms.append(gradient_norm)\n",
    "        points.append(list(point))\n",
    "\n",
    "    return points, gradient_norms\n",
    "\n",
    "\n",
    "fr_points_array, fr_gradient_norms_array = fletcher_rives_method(\n",
    "    ANALYTICAL_GRADIENT,\n",
    "    START_POINT,\n",
    "    ANALYTICAL_HESSE,\n",
    "    0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Fletcher-rives method result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "xₘᵢₙ = [1.02127659574468  -0.0851063829787198]",
      "text/latex": "$\\displaystyle x_{min} = \\left[\\begin{matrix}1.02127659574468 & -0.0851063829787198\\end{matrix}\\right]$"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational.Eq(x_min, Array(fr_points_array[len(fr_points_array) - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "fₘᵢₙ = -0.042553",
      "image/png": "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\n",
      "text/latex": "$\\displaystyle f_{min} = -0.042553$"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational.Eq(f_min, Float(str(target_function(*fr_points_array[len(fr_points_array) - 1])), 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DFP method\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def calculate_new_matrix_a(prev_matrix, delta_x, delta_g):\n",
    "    a = Matrix(prev_matrix)\n",
    "    x = Matrix(delta_x)\n",
    "    g = Matrix(delta_g)\n",
    "\n",
    "    return np.array(\n",
    "        a +\n",
    "        ((x * x.transpose()) / (x.transpose() * g)[0]) -\n",
    "        (((a * g) * g.transpose()) * a) / (((g.transpose() * a) * g)[0]))\n",
    "\n",
    "\n",
    "def dfp_method(analytical_gradient, start_point, hesse, epsilon):\n",
    "    \"\"\"\n",
    "    Minimization of function by DFP method\n",
    "    :param analytical_gradient: list of lambdas     (analytical gradient)\n",
    "    :param start_point:         list of numbers     (start point)\n",
    "    :param hesse:               2d list of numbers  (hesse's matrix)\n",
    "    :param epsilon:             float               (accuracy)\n",
    "    :return: list of points and list of gradients norms for each iteration\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    gradient_norms = []\n",
    "    point = np.array(start_point.copy())\n",
    "    gradient = np.array(calculate_gradient(analytical_gradient, point))\n",
    "    a_matrix = np.array(np.eye(len(analytical_gradient)))\n",
    "\n",
    "    direction = -1 * a_matrix.dot(gradient)\n",
    "    step = calculate_optimal_step(direction, gradient, hesse)\n",
    "    gradient_norm = calculate_gradient_norm(gradient)\n",
    "\n",
    "    points.append(point)\n",
    "    gradient_norms.append(gradient_norm)\n",
    "\n",
    "    while gradient_norm > epsilon:\n",
    "        new_point =  np.add(np.array(point), (step * np.array(direction)))\n",
    "        new_gradient = np.array(calculate_gradient(analytical_gradient, new_point))\n",
    "\n",
    "        delta_g = new_gradient - gradient\n",
    "        delta_x = new_point - point\n",
    "\n",
    "        a_matrix = np.array(calculate_new_matrix_a(a_matrix, delta_x, delta_g))\n",
    "        direction = -1 * a_matrix.dot(new_gradient)\n",
    "\n",
    "        gradient = new_gradient\n",
    "        gradient_norm = calculate_gradient_norm(new_gradient)\n",
    "        step = calculate_optimal_step(direction, gradient, hesse)\n",
    "        point = new_point\n",
    "\n",
    "        gradient_norms.append(gradient_norm)\n",
    "        points.append(list(point))\n",
    "\n",
    "    return points, gradient_norms\n",
    "\n",
    "dfp_points_array, dfp_norms_array = dfp_method(\n",
    "    ANALYTICAL_GRADIENT,\n",
    "    START_POINT,\n",
    "    calculate_hesse(ANALYTICAL_HESSE, [0, 0]),\n",
    "    0.01\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Fletcher-rives method result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "xₘᵢₙ = [1.02127659574468  -0.0851063829787248]",
      "text/latex": "$\\displaystyle x_{min} = \\left[\\begin{matrix}1.02127659574468 & -0.0851063829787248\\end{matrix}\\right]$"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational.Eq(x_min, Array(dfp_points_array[len(dfp_points_array) - 1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "fₘᵢₙ = -0.042553",
      "image/png": "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\n",
      "text/latex": "$\\displaystyle f_{min} = -0.042553$"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational.Eq(f_min, Float(str(target_function(*dfp_points_array[len(dfp_points_array) - 1])), 5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}